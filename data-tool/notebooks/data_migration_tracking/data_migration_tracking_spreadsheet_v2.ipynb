{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d7d917",
   "metadata": {},
   "source": [
    "# Data Migration Tracking v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5514f3",
   "metadata": {},
   "source": [
    "### Description\n",
    "To track data migration status and filings done after migration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca9e20",
   "metadata": {},
   "source": [
    "#### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fea37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in a cell if you haven't installed these packages\n",
    "!pip install pandas openpyxl sqlalchemy numpy psycopg2-binary python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c185b5",
   "metadata": {},
   "source": [
    "#### Core Infrastructure and Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e21ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text, pool\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from typing import List, Dict, Optional, Tuple, Any, Protocol\n",
    "from datetime import datetime\n",
    "from contextlib import contextmanager\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataSource(ABC):\n",
    "    \"\"\"Abstract base class for all data sources\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, config: Dict[str, Any]):\n",
    "        self.name = name\n",
    "        self.config = config\n",
    "        self.enabled = config.get('enabled', True)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fetch_data(self, corp_nums: List[str], **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Fetch data for given corporation numbers\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_column_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"Return mapping from source columns to standard column names\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def is_enabled(self) -> bool:\n",
    "        \"\"\"Check if this data source is enabled\"\"\"\n",
    "        return self.enabled\n",
    "    \n",
    "    def get_dependencies(self) -> List[str]:\n",
    "        \"\"\"Return list of data sources this one depends on\"\"\"\n",
    "        return self.config.get('dependencies', [])\n",
    "\n",
    "\n",
    "class DatabaseManager:\n",
    "    \"\"\"Reusable database connection manager\"\"\"\n",
    "    \n",
    "    def __init__(self, db_config: Dict[str, str], pool_size: int = 5):\n",
    "        self.config = db_config\n",
    "        self.engine = None\n",
    "        self.pool_size = pool_size\n",
    "        self._setup_engine()\n",
    "    \n",
    "    def _setup_engine(self):\n",
    "        \"\"\"Setup database engine with connection pooling\"\"\"\n",
    "        connection_string = (\n",
    "            f\"postgresql://{self.config['username']}:{self.config['password']}\"\n",
    "            f\"@{self.config['host']}:{self.config['port']}/{self.config['database']}\"\n",
    "        )\n",
    "        \n",
    "        self.engine = create_engine(\n",
    "            connection_string,\n",
    "            poolclass=pool.QueuePool,\n",
    "            pool_size=self.pool_size,\n",
    "            max_overflow=10,\n",
    "            pool_pre_ping=True,\n",
    "            pool_recycle=3600\n",
    "        )\n",
    "    \n",
    "    @contextmanager\n",
    "    def get_connection(self):\n",
    "        \"\"\"Context manager for database connections\"\"\"\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = self.engine.connect()\n",
    "            yield conn\n",
    "        except SQLAlchemyError as e:\n",
    "            logger.error(f\"Database error: {e}\")\n",
    "            if conn:\n",
    "                conn.rollback()\n",
    "            raise\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "    \n",
    "    def execute_query(self, query: str, params: Optional[Dict] = None) -> pd.DataFrame:\n",
    "        \"\"\"Execute query with error handling\"\"\"\n",
    "        try:\n",
    "            with self.get_connection() as conn:\n",
    "                return pd.read_sql(query, conn, params=params)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Query execution failed: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def test_connection(self) -> bool:\n",
    "        \"\"\"Test database connection\"\"\"\n",
    "        try:\n",
    "            with self.get_connection() as conn:\n",
    "                conn.execute(text(\"SELECT 1\"))\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Connection test failed: {e}\")\n",
    "            return False\n",
    "\n",
    "##################################################################################\n",
    "##################################################################################\n",
    "\n",
    "class DatabaseSource(DataSource):\n",
    "    \"\"\"Base class for database-based data sources\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, config: Dict[str, Any]):\n",
    "        super().__init__(name, config)\n",
    "        self.db_manager = None\n",
    "        self._setup_database()\n",
    "    \n",
    "    def _setup_database(self):\n",
    "        \"\"\"Setup database connection\"\"\"\n",
    "        if 'database_config' in self.config:\n",
    "            self.db_manager = DatabaseManager(self.config['database_config'])\n",
    "    \n",
    "    def execute_query(self, query: str, params: Optional[Dict] = None) -> pd.DataFrame:\n",
    "        \"\"\"Execute database query\"\"\"\n",
    "        if not self.db_manager:\n",
    "            logger.error(f\"No database manager for {self.name}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return self.db_manager.execute_query(query, params)\n",
    "\n",
    "logger.info(\"Core infrastructure loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d714e5",
   "metadata": {},
   "source": [
    "#### Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModularConfig:\n",
    "    \"\"\"Configuration manager for modular data sources\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.load_environment()\n",
    "        self.setup_data_sources()\n",
    "        self.setup_constants()\n",
    "    \n",
    "    def load_environment(self):\n",
    "        \"\"\"Load environment variables\"\"\"\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        \n",
    "        # File paths\n",
    "        self.GROUP_TABLE_FOLDER = os.getenv('GROUP_TABLE_FOLDER')\n",
    "        self.GROUP_TABLE_FILE_NAME = os.getenv('GROUP_TABLE_FILE_NAME')\n",
    "        self.OUTPUT_FOLDER = os.getenv('OUTPUT_FOLDER')\n",
    "        self.COLUMN_FOR_CORP_NUM = os.getenv('COLUMN_FOR_CORP_NUM')\n",
    "        \n",
    "        # Database configurations\n",
    "        self.colin_extracts_config = { \n",
    "            'database': os.getenv('COLIN_EXTRACT_DB'),\n",
    "            'host': os.getenv('CE_HOST_URL'),\n",
    "            'username': os.getenv('CE_USERNAME'),\n",
    "            'password': os.getenv('CE_PASSWORD'),\n",
    "            'port': os.getenv('CE_PORT', '5432')\n",
    "        }\n",
    "        \n",
    "        self.lear_config = {\n",
    "            'database': os.getenv('LEAR_DB'),\n",
    "            'host': os.getenv('LEAR_HOST_URL'),\n",
    "            'username': os.getenv('LEAR_USERNAME'),\n",
    "            'password': os.getenv('LEAR_PASSWORD'),\n",
    "            'port': os.getenv('LEAR_PORT', '5432')\n",
    "        }\n",
    "    \n",
    "    def setup_data_sources(self):\n",
    "        \"\"\"Configure data sources - easy to modify for future needs\"\"\"\n",
    "        self.data_sources_config = {\n",
    "            'corp_basic_info': {\n",
    "                'class': 'ColinExtractsCorporationBasicSource',\n",
    "                'enabled': True,\n",
    "                'database_config': self.colin_extracts_config,\n",
    "                'priority': 1,  # Lower number = higher priority\n",
    "                'description': 'Corp type, Admin email from Colin Extracts'\n",
    "            },\n",
    "            'corp_names': {\n",
    "                'class': 'ColinExtractsCorporationNamesSource',\n",
    "                'enabled': True,\n",
    "                'database_config': self.colin_extracts_config,\n",
    "                'priority': 2,\n",
    "                'description': 'Corporation names from Colin Extracts'\n",
    "            },\n",
    "            'migration_status': {\n",
    "                'class': 'ColinExtractsMigrationStatusSource',\n",
    "                'enabled': True,\n",
    "                'database_config': self.colin_extracts_config,\n",
    "                'priority': 3,\n",
    "                'description': 'Migration processing status from Colin Extracts'\n",
    "            },\n",
    "            'lear_filings': {\n",
    "                'class': 'LearFilingsSource',\n",
    "                'enabled': True,\n",
    "                'database_config': self.lear_config,\n",
    "                'priority': 4,\n",
    "                'description': 'Post-migration filings from LEAR'\n",
    "            },\n",
    "            # Future data sources can be easily added here:\n",
    "            # 'corp_addresses': {\n",
    "            #     'class': 'ColinCorporationAddressSource',\n",
    "            #     'enabled': True,\n",
    "            #     'database_config': self.colin_config,\n",
    "            #     'priority': 5,\n",
    "            #     'dependencies': ['corp_basic_info']\n",
    "            #     'description': 'Corporation addresses from Colin Extracts'\n",
    "            # },\n",
    "            # 'directors': {\n",
    "            #     'class': 'ColinDirectorsSource', \n",
    "            #     'enabled': False,  # Can be disabled easily\n",
    "            #     'database_config': self.colin_config,\n",
    "            #     'priority': 6,\n",
    "            #     'description': 'Director information from Colin Extracts'\n",
    "            # }\n",
    "        }\n",
    "    \n",
    "    def setup_constants(self):\n",
    "        \"\"\"Setup application constants\"\"\"\n",
    "        self.STANDARD_COLUMNS = {\n",
    "            \"corp_num\": \"Incorporation Number\",\n",
    "            \"corp_name\": \"Company Name\", \n",
    "            \"corp_type\": \"Type\",\n",
    "            \"email\": \"Admin Email\",\n",
    "            \"status\": \"Migration Status\",\n",
    "            \"date\": \"Migrated Date\",\n",
    "            \"filings\": \"Filings Done\",\n",
    "            \"filing_date\": \"Last Filing Date\"\n",
    "            # Future columns can be added here:\n",
    "            # \"address\": \"Registered Address\",\n",
    "            # \"directors\": \"Current Directors\"\n",
    "        }\n",
    "        \n",
    "        self.CHUNK_SIZE = 1000\n",
    "        self.PRINT_DIVIDER = \"=\" * 50\n",
    "\n",
    "config = ModularConfig()\n",
    "logger.info(\"Configuration loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fcc769",
   "metadata": {},
   "source": [
    "#### Specific Data Source Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd7699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColinExtractsCorporationBasicSource(DatabaseSource):\n",
    "    \"\"\"Fetches basic corporation information from Colin Extracts\"\"\"\n",
    "    \n",
    "    def fetch_data(self, corp_nums: List[str], **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Fetch basic corporation data from corporation table\"\"\"\n",
    "        if not corp_nums:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        in_str = \"', '\".join(corp_nums)\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            corp_num,\n",
    "            corp_type_cd,\n",
    "            admin_email\n",
    "        FROM public.corporation\n",
    "        WHERE corp_num IN ('{in_str}')\n",
    "        ORDER BY corp_num\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(f\"Fetching basic info for {len(corp_nums)} corporations from Colin Extracts\")\n",
    "        return self.execute_query(query)\n",
    "    \n",
    "    def get_column_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"Map source columns to standard columns\"\"\"\n",
    "        return {\n",
    "            'corp_num': config.STANDARD_COLUMNS['corp_num'],\n",
    "            'corp_type_cd': config.STANDARD_COLUMNS['corp_type'],\n",
    "            'admin_email': config.STANDARD_COLUMNS['email']\n",
    "        }\n",
    "\n",
    "\n",
    "class ColinExtractsCorporationNamesSource(DatabaseSource):\n",
    "    \"\"\"Fetches corporation names from corp_name table\"\"\"\n",
    "    \n",
    "    def fetch_data(self, corp_nums: List[str], **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Fetch corporation names\"\"\"\n",
    "        if not corp_nums:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        in_str = \"', '\".join(corp_nums)\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            corp_num,\n",
    "            corp_name\n",
    "        FROM public.corp_name\n",
    "        WHERE corp_num IN ('{in_str}')\n",
    "        AND corp_name_typ_cd IN ('CO', 'NB')\n",
    "        AND end_event_id IS NULL\n",
    "        ORDER BY corp_num\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(f\"Fetching names for {len(corp_nums)} corporations from Colin Extracts\")\n",
    "        return self.execute_query(query)\n",
    "    \n",
    "    def get_column_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"Map source columns to standard columns\"\"\"\n",
    "        return {\n",
    "            'corp_num': config.STANDARD_COLUMNS['corp_num'],\n",
    "            'corp_name': config.STANDARD_COLUMNS['corp_name']\n",
    "        }\n",
    "\n",
    "\n",
    "class ColinExtractsMigrationStatusSource(DatabaseSource):\n",
    "    \"\"\"Fetches migration status and date from corp_processing table\"\"\"\n",
    "    \n",
    "    def fetch_data(self, corp_nums: List[str], **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Fetch migration processing status\"\"\"\n",
    "        if not corp_nums:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        in_str = \"', '\".join(corp_nums)\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            corp_num,\n",
    "            processed_status,\n",
    "            create_date\n",
    "        FROM public.corp_processing\n",
    "        WHERE corp_num IN ('{in_str}')\n",
    "        ORDER BY corp_num\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(f\"Fetching migration status for {len(corp_nums)} corporations from Colin Extracts\")\n",
    "        df = self.execute_query(query)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if not df.empty:\n",
    "            df['processed_status'] = df['processed_status'].apply(\n",
    "                lambda x: 'Migrated' if x == 'COMPLETED' else 'Pending'\n",
    "            )\n",
    "            df['create_date'] = pd.to_datetime(df['create_date'], errors='coerce').dt.date\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_column_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"Map source columns to standard columns\"\"\"\n",
    "        return {\n",
    "            'corp_num': config.STANDARD_COLUMNS['corp_num'],\n",
    "            'processed_status': config.STANDARD_COLUMNS['status'],\n",
    "            'create_date': config.STANDARD_COLUMNS['date']\n",
    "        }\n",
    "\n",
    "\n",
    "class LearFilingsSource(DatabaseSource):\n",
    "    \"\"\"Fetches lear filings and last filing date from LEAR\"\"\"\n",
    "    \n",
    "    def fetch_data(self, corp_nums: List[str], **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Fetch LEAR filing data\"\"\"\n",
    "        if not corp_nums:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        in_str = \"', '\".join(corp_nums)\n",
    "        query = f\"\"\"\n",
    "        WITH business_filings AS (\n",
    "            SELECT \n",
    "                b.identifier,\n",
    "                f.filing_date,\n",
    "                f.filing_type,\n",
    "                f.status,\n",
    "                ROW_NUMBER() OVER (PARTITION BY b.identifier ORDER BY f.filing_date DESC) as rn\n",
    "            FROM public.businesses b\n",
    "            JOIN public.filings f ON b.id = f.business_id\n",
    "            WHERE b.identifier IN ('{in_str}')\n",
    "            AND f.source = 'LEAR'\n",
    "            AND f.status = 'COMPLETED'\n",
    "        ),\n",
    "        aggregated_filings AS (\n",
    "            SELECT \n",
    "                identifier,\n",
    "                STRING_AGG(\n",
    "                    DISTINCT filing_type, \n",
    "                    ', '\n",
    "                ) as filings_done,\n",
    "                MAX(filing_date) as last_filing_date\n",
    "            FROM business_filings\n",
    "            GROUP BY identifier\n",
    "        )\n",
    "        SELECT * FROM aggregated_filings\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(f\"Fetching LEAR filings for {len(corp_nums)} corporations\")\n",
    "        df = self.execute_query(query)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if not df.empty:\n",
    "            df['last_filing_date'] = pd.to_datetime(df['last_filing_date'], errors='coerce').dt.date\n",
    "            df['filings_done'] = df['filings_done'].apply(self._convert_filings_to_title_case)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_column_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"Map source columns to standard columns\"\"\"\n",
    "        return {\n",
    "            'identifier': config.STANDARD_COLUMNS['corp_num'],\n",
    "            'filings_done': config.STANDARD_COLUMNS['filings'],\n",
    "            'last_filing_date': config.STANDARD_COLUMNS['filing_date']\n",
    "        }\n",
    "    \n",
    "    def _convert_filings_to_title_case(self, filings_string: str) -> str:\n",
    "        \"\"\"Convert camelCase filing types to Title Case\"\"\"\n",
    "        import re\n",
    "        result = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', filings_string)\n",
    "        return result.title()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7211d",
   "metadata": {},
   "source": [
    "#### Data Source Registry and Manage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1888d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSourceRegistry:\n",
    "    \"\"\"Registry for managing all data sources\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sources = {}\n",
    "        self.source_classes = {\n",
    "            'ColinExtractsCorporationBasicSource': ColinExtractsCorporationBasicSource,\n",
    "            'ColinExtractsCorporationNamesSource': ColinExtractsCorporationNamesSource,\n",
    "            'ColinExtractsMigrationStatusSource': ColinExtractsMigrationStatusSource,\n",
    "            'LearFilingsSource': LearFilingsSource\n",
    "            # Future data source classes can be registered here\n",
    "        }\n",
    "    \n",
    "    def register_source(self, name: str, source_config: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Register a data source\"\"\"\n",
    "        try:\n",
    "            class_name = source_config['class']\n",
    "            if class_name not in self.source_classes:\n",
    "                logger.error(f\"Unknown data source class: {class_name}\")\n",
    "                return False\n",
    "            \n",
    "            source_class = self.source_classes[class_name]\n",
    "            source_instance = source_class(name, source_config)\n",
    "            \n",
    "            self.sources[name] = source_instance\n",
    "            logger.info(f\"Registered data source: {name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to register data source {name}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_source(self, name: str) -> Optional[DataSource]:\n",
    "        \"\"\"Get a data source by name\"\"\"\n",
    "        return self.sources.get(name)\n",
    "    \n",
    "    def get_enabled_sources(self) -> List[Tuple[str, DataSource]]:\n",
    "        \"\"\"Get all enabled data sources sorted by priority\"\"\"\n",
    "        enabled_sources = [\n",
    "            (name, source) for name, source in self.sources.items() \n",
    "            if source.is_enabled()\n",
    "        ]\n",
    "        \n",
    "        # Sort by priority (lower number = higher priority)\n",
    "        enabled_sources.sort(key=lambda x: x[1].config.get('priority', 999))\n",
    "        return enabled_sources\n",
    "    \n",
    "    def validate_dependencies(self) -> bool:\n",
    "        \"\"\"Validate that all dependencies are met\"\"\"\n",
    "        for name, source in self.sources.items():\n",
    "            dependencies = source.get_dependencies()\n",
    "            for dep in dependencies:\n",
    "                if dep not in self.sources:\n",
    "                    logger.error(f\"Data source {name} depends on {dep} which is not registered\")\n",
    "                    return False\n",
    "                if not self.sources[dep].is_enabled():\n",
    "                    logger.error(f\"Data source {name} depends on {dep} which is disabled\")\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "\n",
    "class DataSourceManager:\n",
    "    \"\"\"Manages data collection from multiple sources\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModularConfig):\n",
    "        self.config = config\n",
    "        self.registry = DataSourceRegistry()\n",
    "        self._setup_sources()\n",
    "    \n",
    "    def _setup_sources(self):\n",
    "        \"\"\"Setup all configured data sources\"\"\"\n",
    "        for name, source_config in self.config.data_sources_config.items():\n",
    "            self.registry.register_source(name, source_config)\n",
    "        \n",
    "        if not self.registry.validate_dependencies():\n",
    "            raise ValueError(\"Data source dependency validation failed\")\n",
    "    \n",
    "    def fetch_all_data(self, corp_nums: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Fetch data from all enabled sources and merge\"\"\"\n",
    "        if not corp_nums:\n",
    "            logger.error(\"No corporation numbers provided\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Start with base dataframe\n",
    "        main_df = pd.DataFrame({\n",
    "            config.STANDARD_COLUMNS['corp_num']: corp_nums\n",
    "        })\n",
    "        \n",
    "        enabled_sources = self.registry.get_enabled_sources()\n",
    "        logger.info(f\"Processing {len(enabled_sources)} enabled data sources\")\n",
    "        \n",
    "        for source_name, source in enabled_sources:\n",
    "            try:\n",
    "                logger.info(f\"Fetching data from: {source_name}\")\n",
    "                \n",
    "                # Process in chunks if needed\n",
    "                all_data = []\n",
    "                for i in range(0, len(corp_nums), self.config.CHUNK_SIZE):\n",
    "                    chunk = corp_nums[i:i + self.config.CHUNK_SIZE]\n",
    "                    chunk_data = source.fetch_data(chunk)\n",
    "                    if not chunk_data.empty:\n",
    "                        all_data.append(chunk_data)\n",
    "                \n",
    "                if all_data:\n",
    "                    source_df = pd.concat(all_data, ignore_index=True)\n",
    "                    \n",
    "                    # Apply column mapping\n",
    "                    column_mapping = source.get_column_mapping()\n",
    "                    source_df = source_df.rename(columns=column_mapping)\n",
    "                    \n",
    "                    # Merge with main dataframe\n",
    "                    merge_column = config.STANDARD_COLUMNS['corp_num']\n",
    "                    main_df = main_df.merge(source_df, on=merge_column, how='left')\n",
    "                    \n",
    "                    logger.info(f\"Successfully merged data from {source_name}\")\n",
    "                else:\n",
    "                    logger.warning(f\"No data returned from {source_name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to fetch data from {source_name}: {e}\")\n",
    "                # Continue with other sources even if one fails\n",
    "                continue\n",
    "        \n",
    "        # Fill missing values for optional columns\n",
    "        optional_columns = [\n",
    "            config.STANDARD_COLUMNS['filings'],\n",
    "            config.STANDARD_COLUMNS['filing_date']\n",
    "        ]\n",
    "        \n",
    "        for col in optional_columns:\n",
    "            if col in main_df.columns:\n",
    "                main_df[col] = main_df[col].fillna('')\n",
    "                if 'date' in col.lower():\n",
    "                    main_df[col] = main_df[col].astype(str).replace('NaT', '')\n",
    "        \n",
    "        logger.info(f\"Final dataset shape: {main_df.shape}\")\n",
    "        return main_df\n",
    "    \n",
    "    def add_data_source(self, name: str, source_config: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Dynamically add a new data source\"\"\"\n",
    "        return self.registry.register_source(name, source_config)\n",
    "    \n",
    "    def disable_data_source(self, name: str) -> bool:\n",
    "        \"\"\"Disable a data source\"\"\"\n",
    "        source = self.registry.get_source(name)\n",
    "        if source:\n",
    "            source.enabled = False\n",
    "            logger.info(f\"Disabled data source: {name}\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def enable_data_source(self, name: str) -> bool:\n",
    "        \"\"\"Enable a data source\"\"\"\n",
    "        source = self.registry.get_source(name)\n",
    "        if source:\n",
    "            source.enabled = True\n",
    "            logger.info(f\"Enabled data source: {name}\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def list_sources(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"List all registered data sources with their status\"\"\"\n",
    "        result = {}\n",
    "        for name, source in self.registry.sources.items():\n",
    "            result[name] = {\n",
    "                'enabled': source.is_enabled(),\n",
    "                'priority': source.config.get('priority', 999),\n",
    "                'description': source.config.get('description', ''),\n",
    "                'dependencies': source.get_dependencies()\n",
    "            }\n",
    "        return result\n",
    "\n",
    "# Initialize the data source manager\n",
    "data_manager = DataSourceManager(config)\n",
    "\n",
    "# Display registered sources\n",
    "print(\"Registered Data Sources:\")\n",
    "print(\"=\" * 50)\n",
    "for name, info in data_manager.list_sources().items():\n",
    "    status = \"✅ ENABLED\" if info['enabled'] else \"❌ DISABLED\"\n",
    "    print(f\"{name}: {status}\")\n",
    "    print(f\"  Priority: {info['priority']}\")\n",
    "    print(f\"  Description: {info['description']}\")\n",
    "    if info['dependencies']:\n",
    "        print(f\"  Dependencies: {', '.join(info['dependencies'])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0591a4c",
   "metadata": {},
   "source": [
    "#### Main Execution Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MigrationTrackingEngine:\n",
    "    \"\"\"Main engine for the migration tracking system\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModularConfig, data_manager: DataSourceManager):\n",
    "        self.config = config\n",
    "        self.data_manager = data_manager\n",
    "    \n",
    "    def load_corporation_numbers(self) -> List[str]:\n",
    "        \"\"\"Load corporation numbers from Excel file\"\"\"\n",
    "        try:\n",
    "            file_path = f\"{self.config.GROUP_TABLE_FOLDER}/{self.config.GROUP_TABLE_FILE_NAME}\"\n",
    "            logger.info(f\"Loading corporation numbers from: {file_path}\")\n",
    "            \n",
    "            df = pd.read_excel(\n",
    "                file_path, \n",
    "                sheet_name=\"Sheet1\", \n",
    "                usecols=[self.config.COLUMN_FOR_CORP_NUM]\n",
    "            )\n",
    "            \n",
    "            corp_nums = df[self.config.COLUMN_FOR_CORP_NUM].dropna().astype(str).tolist()\n",
    "            logger.info(f\"Loaded {len(corp_nums)} corporation numbers\")\n",
    "            return corp_nums\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load corporation numbers: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def run_full_process(self) -> pd.DataFrame:\n",
    "        \"\"\"Run the complete migration tracking process\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting modular migration tracking process\")\n",
    "            logger.info(self.config.PRINT_DIVIDER)\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Step 1: Load corporation numbers\n",
    "            corp_nums = self.load_corporation_numbers()\n",
    "            if not corp_nums:\n",
    "                logger.error(\"No corporation numbers loaded\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Step 2: Fetch all data using modular sources\n",
    "            logger.info(\"Fetching data from all configured sources...\")\n",
    "            result_df = self.data_manager.fetch_all_data(corp_nums)\n",
    "            \n",
    "            if result_df.empty:\n",
    "                logger.error(\"No data retrieved\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Step 3: Export results\n",
    "            output_file = self._export_results(result_df)\n",
    "            \n",
    "            # Step 4: Generate summary\n",
    "            end_time = datetime.now()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            self._print_summary(result_df, output_file, duration)\n",
    "            \n",
    "            return result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Process execution failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _export_results(self, df: pd.DataFrame) -> str:\n",
    "        \"\"\"Export results to Excel\"\"\"\n",
    "        from openpyxl.styles import Font, Alignment\n",
    "        \n",
    "        # Prepare filename\n",
    "        output_path = f\"{self.config.OUTPUT_FOLDER}/migration_tracking.xlsx\"\n",
    "        final_path = self._generate_unique_filename(output_path)\n",
    "        \n",
    "        # Export with formatting\n",
    "        with pd.ExcelWriter(final_path, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name='Migration Tracking', index=False)\n",
    "            \n",
    "            worksheet = writer.sheets['Migration Tracking']\n",
    "\n",
    "            # Freeze the first row\n",
    "            worksheet.freeze_panes = 'A2'\n",
    "            \n",
    "            # Format header\n",
    "            for cell in worksheet[1]:\n",
    "                cell.font = Font(bold=True)\n",
    "            \n",
    "            # Adjust column widths\n",
    "            for column in worksheet.columns:\n",
    "                max_length = 0\n",
    "                column_letter = column[0].column_letter\n",
    "                \n",
    "                for cell in column:\n",
    "                    try:\n",
    "                        cell_length = len(str(cell.value))\n",
    "                        if cell_length > max_length:\n",
    "                            max_length = cell_length\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                adjusted_width = min(max_length + 2, 50)\n",
    "                worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "        \n",
    "        logger.info(f\"Results exported to: {final_path}\")\n",
    "        return final_path\n",
    "    \n",
    "    def _generate_unique_filename(self, original_path: str) -> str:\n",
    "        \"\"\"Generate unique filename with timestamp\"\"\"\n",
    "        directory = os.path.dirname(original_path)\n",
    "        filename = os.path.basename(original_path)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        new_filename = f\"{name}_{timestamp}{ext}\"\n",
    "        return os.path.join(directory, new_filename)\n",
    "    \n",
    "    def _print_summary(self, df: pd.DataFrame, output_file: str, duration):\n",
    "        \"\"\"Print process summary\"\"\"\n",
    "        print(f\"\\n{self.config.PRINT_DIVIDER}\")\n",
    "        print(\"MIGRATION TRACKING COMPLETED\")\n",
    "        print(f\"{self.config.PRINT_DIVIDER}\")\n",
    "        \n",
    "        # Basic stats\n",
    "        total_corps = len(df)\n",
    "        migrated_corps = len(df[df[self.config.STANDARD_COLUMNS['status']] == 'Migrated']) if self.config.STANDARD_COLUMNS['status'] in df.columns else 0\n",
    "        corps_with_filings = len(df[df[self.config.STANDARD_COLUMNS['filings']] != '']) if self.config.STANDARD_COLUMNS['filings'] in df.columns else 0\n",
    "        \n",
    "        print(f\"Results Summary:\")\n",
    "        print(f\"   Total Corporations: {total_corps}\")\n",
    "        print(f\"   Migrated: {migrated_corps}\")\n",
    "        print(f\"   With Filings: {corps_with_filings}\")\n",
    "        print(f\"   Processing Time: {duration}\")\n",
    "        print(f\"   Output File: {output_file}\")\n",
    "        \n",
    "        # Data sources used\n",
    "        print(f\"\\nData Sources Used:\")\n",
    "        enabled_sources = self.data_manager.registry.get_enabled_sources()\n",
    "        for name, source in enabled_sources:\n",
    "            print(f\"   ✅ {name}: {source.config.get('description', 'No description')}\")\n",
    "        \n",
    "        print(f\"\\n{self.config.PRINT_DIVIDER}\")\n",
    "\n",
    "# Initialize the engine\n",
    "engine = MigrationTrackingEngine(config, data_manager)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6affbb6c",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a5320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_execution(mode: str) -> None:\n",
    "    \"\"\"The main execution of migration tracking\"\"\"\n",
    "    if not mode or mode not in ['prod', 'test']:\n",
    "         print(\"Invalid mode, current available modes: prod, test\")\n",
    "    try:\n",
    "            # Run the complete process\n",
    "            result_df = engine.run_full_process()\n",
    "            \n",
    "            if not result_df.empty:\n",
    "                if mode == 'prod':\n",
    "                    print(\"\\nSample Results:\")\n",
    "                    display(result_df.head(10))\n",
    "                elif mode == 'test':\n",
    "                    print(\"\\nTest mode, display full dataframe:\")\n",
    "                    with pd.option_context(\"display.max_rows\", None):\n",
    "                        display(result_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Main execution failed: {e}\")\n",
    "\n",
    "#############################################################\n",
    "main_execution('test')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
