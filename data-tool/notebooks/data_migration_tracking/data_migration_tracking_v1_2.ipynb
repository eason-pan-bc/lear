{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2da51c2",
   "metadata": {},
   "source": [
    "# Data Migration Tracking - Optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab59fb5",
   "metadata": {},
   "source": [
    "### Description\n",
    "To track data migration status and filings done after migration - Optimized version with fewer queries and dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5329a27",
   "metadata": {},
   "source": [
    "#### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d62138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you haven't installed these packages\n",
    "!pip install pandas openpyxl sqlalchemy numpy psycopg2-binary python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630412d",
   "metadata": {},
   "source": [
    "### Setup and COnfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255cee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from typing import List, Any\n",
    "from openpyxl.styles import Font, Alignment\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "GROUP_TABLE_FOLDER = os.getenv('GROUP_TABLE_FOLDER')\n",
    "GROUP_TABLE_FILE_NAME = os.getenv('GROUP_TABLE_FILE_NAME')\n",
    "OUTPUT_FOLDER = os.getenv('OUTPUT_FOLDER')\n",
    "COLUMN_FOR_CORP_NUM = os.getenv('COLUMN_FOR_CORP_NUM')\n",
    "\n",
    "# Colin Extracts DB\n",
    "COLIN_EXTRACT_DB = os.getenv('COLIN_EXTRACT_DB')\n",
    "CE_HOST_URL = os.getenv('CE_HOST_URL')\n",
    "CE_USERNAME = os.getenv('CE_USERNAME')\n",
    "CE_PASSWORD = os.getenv('CE_PASSWORD')\n",
    "CE_PORT = os.getenv('CE_PORT')\n",
    "\n",
    "# Lear DB\n",
    "LEAR_DB = os.getenv('LEAR_DB')\n",
    "LEAR_HOST_URL = os.getenv('LEAR_HOST_URL')\n",
    "LEAR_USERNAME = os.getenv('LEAR_USERNAME')\n",
    "LEAR_PASSWORD = os.getenv('LEAR_PASSWORD')\n",
    "LEAR_PORT = os.getenv('LEAR_PORT')\n",
    "\n",
    "# Column mapping\n",
    "COLUMN_NAMES = {\n",
    "    \"corp_num\": \"Incorporation Number\",\n",
    "    \"corp_name\": \"Company Name\",\n",
    "    \"corp_type\": \"Type\",\n",
    "    \"email\": \"Admin Email\",\n",
    "    \"status\": \"Migration Status\",\n",
    "    \"date\": \"Migrated Date\",\n",
    "    \"filings\": \"Filings Done\",\n",
    "    \"filing_date\": \"Last Filing Date\"\n",
    "}\n",
    "\n",
    "PRINT_DIVIDER = \"=\" * 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998dd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_connection_string(host_address: str, database: str, user_name: str, db_password: str, port: str = \"5432\") -> str:\n",
    "    \"\"\"Create db connection string.\"\"\"\n",
    "    return f\"postgresql://{user_name}:{db_password}@{host_address}:{port}/{database}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7892f921",
   "metadata": {},
   "source": [
    "### Read Excel file and extract corp_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e17a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    full_group_table_path = f\"{GROUP_TABLE_FOLDER}/{GROUP_TABLE_FILE_NAME}\"\n",
    "    corp_nums_df = pd.read_excel(full_group_table_path, sheet_name=\"Sheet1\", usecols=[COLUMN_FOR_CORP_NUM])\n",
    "    corp_nums_df = corp_nums_df.sort_values(COLUMN_FOR_CORP_NUM)\n",
    "    \n",
    "    corp_num_column_values = corp_nums_df[COLUMN_FOR_CORP_NUM].dropna().tolist()\n",
    "    \n",
    "    print(f\"Found {len(corp_num_column_values)} corps in the group table\")\n",
    "    print(f\"All corps: {corp_num_column_values}\")\n",
    "    print(PRINT_DIVIDER)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading Excel file: {e}\")\n",
    "    corp_num_column_values = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabe69b2",
   "metadata": {},
   "source": [
    "### Connect to Colin Extracts db and get all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    colin_extracts_connection_string = get_db_connection_string(\n",
    "        CE_HOST_URL, COLIN_EXTRACT_DB, CE_USERNAME, CE_PASSWORD, CE_PORT\n",
    "    )\n",
    "    colin_extracts_engine = create_engine(colin_extracts_connection_string)\n",
    "    \n",
    "    # Test connection\n",
    "    with colin_extracts_engine.connect() as conn:\n",
    "        conn.execute(text(\"SELECT 1\"))\n",
    "    print(\"Colin Extracts database connection successful\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Connection to Colin Extracts failed: {e}\")\n",
    "    colin_extracts_engine = None\n",
    "\n",
    "print(PRINT_DIVIDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ba4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get corp_name, corp_type, admin_email, migration_status, migration_date from Colin Extracts\n",
    "if colin_extracts_engine and corp_num_column_values:\n",
    "    try:\n",
    "        values_str = \"', '\".join(str(val) for val in corp_num_column_values)\n",
    "        in_clause = f\"corp_num IN ('{values_str}')\"\n",
    "\n",
    "        # Single comprehensive query joining all needed tables\n",
    "        combined_query = f\"\"\"\n",
    "        SELECT \n",
    "            corp.corp_num as \"{COLUMN_NAMES['corp_num']}\",\n",
    "            cn.corp_name as \"{COLUMN_NAMES['corp_name']}\",\n",
    "            corp.corp_type_cd as \"{COLUMN_NAMES['corp_type']}\",\n",
    "            corp.admin_email as \"{COLUMN_NAMES['email']}\",\n",
    "            CASE \n",
    "                WHEN cp.processed_status = 'COMPLETED' THEN 'Migrated'\n",
    "                ELSE 'Pending'\n",
    "            END as \"{COLUMN_NAMES['status']}\",\n",
    "            cp.create_date::date as \"{COLUMN_NAMES['date']}\"\n",
    "        FROM public.corporation corp\n",
    "        LEFT JOIN public.corp_name cn ON corp.corp_num = cn.corp_num \n",
    "            AND cn.corp_name_typ_cd IN ('CO', 'NB') \n",
    "            AND cn.end_event_id IS NULL\n",
    "        LEFT JOIN public.corp_processing cp ON corp.corp_num = cp.corp_num\n",
    "        WHERE corp.{in_clause}\n",
    "        ORDER BY corp.corp_num\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute combined query\n",
    "        tracking_df = pd.read_sql(combined_query, colin_extracts_engine)\n",
    "        \n",
    "        # Convert date column\n",
    "        tracking_df[COLUMN_NAMES['date']] = pd.to_datetime(tracking_df[COLUMN_NAMES['date']])\n",
    "        \n",
    "        print(f\"Retrieved data for {len(tracking_df)} corporations from Colin Extracts\")\n",
    "        display(tracking_df.head())\n",
    "        print(PRINT_DIVIDER)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to execute combined query: {e}\")\n",
    "        tracking_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43ad4f",
   "metadata": {},
   "source": [
    "### Connect to LEAR DB and get post-migration filings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to LEAR DB\n",
    "try:\n",
    "    lear_connection_string = get_db_connection_string(\n",
    "        LEAR_HOST_URL, LEAR_DB, LEAR_USERNAME, LEAR_PASSWORD, LEAR_PORT\n",
    "    )\n",
    "    lear_engine = create_engine(lear_connection_string)\n",
    "    \n",
    "    # Test connection\n",
    "    with lear_engine.connect() as lear_conn:\n",
    "        lear_conn.execute(text(\"SELECT 1\"))\n",
    "    print(\"LEAR database connection successful\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Connection to LEAR failed: {e}\")\n",
    "    lear_engine = None\n",
    "\n",
    "print(PRINT_DIVIDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb466da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get post-migration filings data (filing types and last filing date) from LEAR\n",
    "def convert_filings_to_title_case(filings_string: str) -> str:\n",
    "        \"\"\"Convert camelCase to Title Case (e.g., 'annualReport' -> 'Annual Report')\"\"\"\n",
    "        import re\n",
    "        result = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', filings_string)\n",
    "        return result.title()\n",
    "\n",
    "\n",
    "if lear_engine and corp_num_column_values:\n",
    "    try:\n",
    "        values_str = \"', '\".join(str(val) for val in corp_num_column_values)\n",
    "        in_clause = f\"b.identifier IN ('{values_str}')\"\n",
    "        \n",
    "        # Combined query to get business IDs and filings in one go\n",
    "        lear_combined_query = f\"\"\"\n",
    "        WITH filing_summary AS (\n",
    "            SELECT \n",
    "                b.identifier,\n",
    "                STRING_AGG(\n",
    "                    DISTINCT filing_type,\n",
    "                    ', '\n",
    "                ) as filings_done,\n",
    "                MAX(f.filing_date)::date as last_filing_date\n",
    "            FROM public.businesses b\n",
    "            LEFT JOIN public.filings f ON b.id = f.business_id \n",
    "            AND f.source = 'LEAR'\n",
    "            AND f.status = 'COMPLETED'\n",
    "            WHERE {in_clause}\n",
    "            GROUP BY b.identifier\n",
    "        )\n",
    "        SELECT \n",
    "            identifier as \"{COLUMN_NAMES['corp_num']}\",\n",
    "            COALESCE(filings_done, '') as \"{COLUMN_NAMES['filings']}\",\n",
    "            COALESCE(last_filing_date::text, '') as \"{COLUMN_NAMES['filing_date']}\"\n",
    "        FROM filing_summary\n",
    "        ORDER BY identifier\n",
    "        \"\"\"\n",
    "        \n",
    "        lear_filings_df = pd.read_sql(lear_combined_query, lear_engine)\n",
    "        \n",
    "        print(f\"Retrieved LEAR filings data for {len(lear_filings_df)} corporations\")\n",
    "        if not lear_filings_df.empty:\n",
    "            lear_filings_df[COLUMN_NAMES['filings']] = lear_filings_df[COLUMN_NAMES['filings']].apply(convert_filings_to_title_case)\n",
    "            display(lear_filings_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting LEAR filings data: {e}\")\n",
    "        lear_filings_df = pd.DataFrame()\n",
    "    \n",
    "print(PRINT_DIVIDER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f38d5",
   "metadata": {},
   "source": [
    "### Combine All Data and Save Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_filename(original_path: str) -> str:\n",
    "    \"\"\"Generate unique filename with date and incremental number if file exists.\"\"\"\n",
    "    directory = os.path.dirname(original_path)\n",
    "    filename = os.path.basename(original_path)\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    \n",
    "    today = datetime.now().strftime('%Y%m%d')\n",
    "    new_filename = f\"{name}_{today}{ext}\"\n",
    "    new_path = os.path.join(directory, new_filename)\n",
    "    \n",
    "    if not os.path.exists(new_path):\n",
    "        return new_path\n",
    "    \n",
    "    counter = 2\n",
    "    while counter <= 999:\n",
    "        incremental_filename = f\"{name}_{today}_{counter:02d}{ext}\"\n",
    "        incremental_path = os.path.join(directory, incremental_filename)\n",
    "        \n",
    "        if not os.path.exists(incremental_path):\n",
    "            return incremental_path\n",
    "        \n",
    "        counter += 1\n",
    "    \n",
    "    raise Exception(\"Too many files with the same name pattern\")\n",
    "\n",
    "\n",
    "def format_and_save_excel(df: pd.DataFrame, file_save_path: str = 'output.xlsx', sheet_name: str = 'Sheet1') -> None:\n",
    "    \"\"\"Save to Excel file with basic freeze and alignment.\"\"\"\n",
    "    file_save_path = generate_unique_filename(file_save_path)\n",
    "\n",
    "    with pd.ExcelWriter(file_save_path, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "        \n",
    "        # Freeze the 1st row\n",
    "        worksheet.freeze_panes = 'A2'\n",
    "        \n",
    "        # Make header bold and left-align all cells\n",
    "        for cell in worksheet[1]:\n",
    "            cell.font = Font(bold=True)\n",
    "            cell.alignment = Alignment(horizontal='left')\n",
    "        \n",
    "        for row in worksheet.iter_rows(min_row=2):\n",
    "            for cell in row:\n",
    "                cell.alignment = Alignment(horizontal='left')\n",
    "        \n",
    "        # Adjust column widths\n",
    "        for column in worksheet.columns:\n",
    "            max_length = max(len(str(cell.value)) for cell in column if cell.value)\n",
    "            worksheet.column_dimensions[column[0].column_letter].width = max_length + 2\n",
    "    \n",
    "    print(f\"DataFrame saved to {file_save_path} with frozen header and left alignment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if not tracking_df.empty:\n",
    "        # Merge with LEAR filings data\n",
    "        if not lear_filings_df.empty:\n",
    "            final_df = tracking_df.merge(\n",
    "                lear_filings_df,\n",
    "                on=COLUMN_NAMES['corp_num'],\n",
    "                how='left'\n",
    "            )\n",
    "        else:\n",
    "            # If no LEAR data, add empty columns\n",
    "            final_df = tracking_df.copy()\n",
    "            final_df[COLUMN_NAMES['filings']] = ''\n",
    "            final_df[COLUMN_NAMES['filing_date']] = ''\n",
    "        \n",
    "        # Convert date column to string for Excel export\n",
    "        final_df[COLUMN_NAMES['date']] = final_df[COLUMN_NAMES['date']].astype(str)\n",
    "        \n",
    "        print(f\"Final dataset contains {len(final_df)} corporations\")\n",
    "        print(f\"Corporations with filings: {len(final_df[final_df[COLUMN_NAMES['filings']] != ''])}\")\n",
    "        \n",
    "        # Display sample of final data\n",
    "        with pd.option_context('display.max_rows', None):  # Adjust None to a number when dataset is big\n",
    "            display(final_df)\n",
    "        \n",
    "        # Save to Excel\n",
    "        output_path = f\"{OUTPUT_FOLDER}/migration_tracking_result.xlsx\"\n",
    "        format_and_save_excel(final_df, output_path)\n",
    "        \n",
    "        print(PRINT_DIVIDER)\n",
    "        print(\"Processing completed successfully!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No data retrieved from Colin Extracts database\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in final data processing: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
