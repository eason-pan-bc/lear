{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927badec",
   "metadata": {},
   "source": [
    "# Data Migration Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb596e27",
   "metadata": {},
   "source": [
    "### Description\n",
    "To track data migration status and filings done after migration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859fe091",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049dced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in a cell if you haven't installed these packages\n",
    "!pip install pandas openpyxl sqlalchemy numpy psycopg2-binary python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797120f",
   "metadata": {},
   "source": [
    "### Define and get constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4de0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# file path\n",
    "GROUP_TABLE_FOLDER = os.getenv('GROUP_TABLE_FOLDER')\n",
    "GROUP_TABLE_FILE_NAME = os.getenv('GROUP_TABLE_FILE_NAME')\n",
    "OUTPUT_FOLDER = os.getenv('OUTPUT_FOLDER')\n",
    "\n",
    "# corp_num column name\n",
    "COLUMN_FOR_CORP_NUM = os.getenv('COLUMN_FOR_CORP_NUM')\n",
    "\n",
    "# db configs\n",
    "# Colin Extracts\n",
    "COLIN_EXTRACT_DB = os.getenv('COLIN_EXTRACT_DB')\n",
    "CE_HOST_URL = os.getenv('CE_HOST_URL')\n",
    "CE_USERNAME = os.getenv('CE_USERNAME')\n",
    "CE_PASSWORD = os.getenv('CE_PASSWORD')\n",
    "CE_PORT = os.getenv('CE_PORT')\n",
    "# Lear\n",
    "LEAR_DB = os.getenv('LEAR_DB')\n",
    "LEAR_HOST_URL = os.getenv('LEAR_HOST_URL')\n",
    "LEAR_USERNAME = os.getenv('LEAR_USERNAME')\n",
    "LEAR_PASSWORD = os.getenv('LEAR_PASSWORD')\n",
    "LEAR_PORT = os.getenv('LEAR_PORT')\n",
    "\n",
    "# Display stuff\n",
    "PRINT_DIVIDER = \"=\" * 50\n",
    "\n",
    "# Tracking Table Column Names\n",
    "COLUMN_NAMES = {\n",
    "    \"corp_num\": \"Incorporation Number\",\n",
    "    \"corp_name\": \"Company Name\",\n",
    "    \"corp_type\": \"Type\",\n",
    "    \"email\": \"Admin Email\",\n",
    "    \"status\": \"Migration Status\",\n",
    "    \"date\": \"Migrated Date\",\n",
    "    \"filings\": \"Filings Done\",\n",
    "    \"filing_date\": \"Last Filing Date\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d42f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from typing import List, Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to establish database connection\n",
    "def get_db_connection_string(\n",
    "    host_address: str,\n",
    "    database: str,\n",
    "    user_name: str,\n",
    "    db_password: str,\n",
    "    port: str = \"5432\",\n",
    ") -> str:\n",
    "    \"\"\"Create db connection string.\"\"\"\n",
    "    connection_string = (\n",
    "        f\"postgresql://{user_name}:{db_password}@{host_address}:{port}/{database}\"\n",
    "    )\n",
    "\n",
    "    return connection_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0964d1f1",
   "metadata": {},
   "source": [
    "### Read Excel file and extract corp_nums and initialize the tracking dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    full_group_table_path = f\"{GROUP_TABLE_FOLDER}/{GROUP_TABLE_FILE_NAME}\"\n",
    "    corp_nums_df = pd.read_excel(\n",
    "        full_group_table_path, sheet_name=\"Sheet1\", usecols=[COLUMN_FOR_CORP_NUM]\n",
    "    )\n",
    "    corp_nums_df = corp_nums_df.sort_values(COLUMN_FOR_CORP_NUM)\n",
    "\n",
    "    corp_num_column_values = corp_nums_df[COLUMN_FOR_CORP_NUM].dropna().tolist()\n",
    "\n",
    "    corp_nums_df = corp_nums_df.rename(\n",
    "        columns={COLUMN_FOR_CORP_NUM: COLUMN_NAMES[\"corp_num\"]}\n",
    "    )\n",
    "    print(\"Shape of data - Groups\", corp_nums_df.shape)\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(corp_nums_df.head())\n",
    "\n",
    "    print(PRINT_DIVIDER)\n",
    "    print(f\"Found {len(corp_num_column_values)} corps in the group table\")\n",
    "    print(f\"All corps:\\n{corp_num_column_values}\")\n",
    "    print(PRINT_DIVIDER)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading Excel file: {e}\")\n",
    "    corp_num_column_values = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e181e771",
   "metadata": {},
   "source": [
    "### Get Data from Colin Extracts Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93209ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Connect to Colin Extracts DB\n",
    "try:\n",
    "    colin_extracts_connection_string = get_db_connection_string(\n",
    "        CE_HOST_URL, COLIN_EXTRACT_DB, CE_USERNAME, CE_PASSWORD, CE_PORT\n",
    "    )\n",
    "    colin_extracts_engine = create_engine(colin_extracts_connection_string)\n",
    "\n",
    "    # test connection\n",
    "    with colin_extracts_engine.connect() as conn:\n",
    "        conn.execute(text(\"SELECT 1\"))\n",
    "    print(\"Colin Extracts database connection successful\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Connection to Colin Extracts failed: {e}\")\n",
    "    colin_extracts_engine = None\n",
    "\n",
    "print(PRINT_DIVIDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8eff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Get corp names\n",
    "if colin_extracts_engine and corp_num_column_values:\n",
    "    try:\n",
    "        values_str = \"', '\".join(str(val) for val in corp_num_column_values)\n",
    "        in_clause = f\"corp_num IN ('{values_str}')\"\n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT corp_num, corp_name\n",
    "        FROM public.corp_name\n",
    "        WHERE {in_clause}\n",
    "        AND corp_name_typ_cd IN ('CO', 'NB')\n",
    "        AND end_event_id IS NULL\n",
    "        ORDER BY corp_num\n",
    "        \"\"\"\n",
    "        corp_names_data = pd.read_sql(query, colin_extracts_engine)\n",
    "        print(f\"Found {len(corp_names_data)} matches.\")\n",
    "        print(PRINT_DIVIDER)\n",
    "\n",
    "        add_name_df = corp_nums_df.copy(deep=True)\n",
    "        add_name_df = add_name_df.merge(\n",
    "            corp_names_data,\n",
    "            left_on=COLUMN_NAMES[\"corp_num\"],\n",
    "            right_on=\"corp_num\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        add_name_df = add_name_df.drop(\"corp_num\", axis=1)\n",
    "        add_name_df = add_name_df.rename(\n",
    "            columns={\"corp_name\": COLUMN_NAMES[\"corp_name\"]}\n",
    "        )\n",
    "\n",
    "        print(f\"Total {len(add_name_df)} rows.\")\n",
    "        with pd.option_context(\"display.max_rows\", None):\n",
    "            display(add_name_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to execute queries: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fccb95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Get Corp types and admin_email from corporation table\n",
    "if colin_extracts_engine and corp_num_column_values:\n",
    "    try:\n",
    "        values_str = \"', '\".join(str(val) for val in corp_num_column_values)\n",
    "        in_clause = f\"corp_num IN ('{values_str}')\"\n",
    "\n",
    "        type_email_query = f\"\"\"\n",
    "        SELECT corp_num, corp_type_cd, admin_email\n",
    "        FROM public.corporation\n",
    "        WHERE {in_clause}\n",
    "        ORDER BY corp_num\n",
    "        \"\"\"\n",
    "        type_email_data = pd.read_sql(type_email_query, colin_extracts_engine)\n",
    "        print(f\"Found {len(type_email_data)} matches.\")\n",
    "        print(PRINT_DIVIDER)\n",
    "\n",
    "        add_type_email_df = add_name_df.copy(deep=True)\n",
    "        add_type_email_df = add_type_email_df.merge(\n",
    "            type_email_data,\n",
    "            left_on=COLUMN_NAMES[\"corp_num\"],\n",
    "            right_on=\"corp_num\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        add_type_email_df = add_type_email_df.drop(\"corp_num\", axis=1)\n",
    "        add_type_email_df = add_type_email_df.rename(\n",
    "            columns={\n",
    "                \"corp_type_cd\": COLUMN_NAMES[\"corp_type\"],\n",
    "                \"admin_email\": COLUMN_NAMES[\"email\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"Total {len(add_type_email_df)} rows.\")\n",
    "        with pd.option_context(\"display.max_rows\", None):\n",
    "            display(add_type_email_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to execute queries: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Get status and migrated date from corp_processing table\n",
    "if colin_extracts_engine and corp_num_column_values:\n",
    "    try:\n",
    "        values_str = \"', '\".join(str(val) for val in corp_num_column_values)\n",
    "        in_clause = f\"corp_num IN ('{values_str}')\"\n",
    "\n",
    "        migration_query = f\"\"\"\n",
    "        SELECT corp_num, processed_status, create_date\n",
    "        FROM public.corp_processing\n",
    "        WHERE {in_clause}\n",
    "        ORDER BY corp_num\n",
    "        \"\"\"\n",
    "        migration_data = pd.read_sql(migration_query, colin_extracts_engine)\n",
    "        print(f\"Found {len(migration_data)} matches.\")\n",
    "        print(PRINT_DIVIDER)\n",
    "\n",
    "        migration_df = add_type_email_df.copy(deep=True)\n",
    "        migration_df = migration_df.merge(\n",
    "            migration_data,\n",
    "            left_on=COLUMN_NAMES[\"corp_num\"],\n",
    "            right_on=\"corp_num\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        migration_df = migration_df.drop(\"corp_num\", axis=1)\n",
    "        migration_df = migration_df.rename(\n",
    "            columns={\n",
    "                \"processed_status\": COLUMN_NAMES['status'],\n",
    "                \"create_date\": COLUMN_NAMES['date'],\n",
    "            }\n",
    "        )\n",
    "        migration_df[COLUMN_NAMES['status']] = np.where(migration_df[COLUMN_NAMES['status']] == 'COMPLETED', 'Migrated', 'Pending')\n",
    "        migration_df[COLUMN_NAMES['date']] = pd.to_datetime(migration_df[COLUMN_NAMES['date']].dt.date)\n",
    "\n",
    "        print(f\"Total {len(migration_df)} rows.\")\n",
    "        with pd.option_context(\"display.max_rows\", None):\n",
    "            display(migration_df)\n",
    "        print(PRINT_DIVIDER)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to execute queries: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73669f10",
   "metadata": {},
   "source": [
    "### Get Data from LEAR DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b07531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to LEAR DB\n",
    "try:\n",
    "    lear_connection_string = get_db_connection_string(\n",
    "        LEAR_HOST_URL, LEAR_DB, LEAR_USERNAME, LEAR_PASSWORD, LEAR_PORT\n",
    "    )\n",
    "    lear_engine = create_engine(lear_connection_string)\n",
    "\n",
    "    # test connection\n",
    "    with lear_engine.connect() as lear_conn:\n",
    "        lear_conn.execute(text(\"SELECT 1\"))\n",
    "    print(\"LEAR database connection successful\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Connection to LEAR failed: {e}\")\n",
    "    lear_engine = None\n",
    "\n",
    "print(PRINT_DIVIDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get LEAR filings and the latest filing date\n",
    "\n",
    "def get_business_ids(db_connection: Any, corp_nums_list: List) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query the Businesses table to get business IDs for given corp_nums_list\n",
    "    \"\"\"\n",
    "    if not corp_nums_list:\n",
    "        print(\"Empty corp_nums_list\")\n",
    "        print(PRINT_DIVIDER)\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    values_str = \"', '\".join(str(val) for val in corp_nums_list)\n",
    "    in_clause = f\"identifier IN ('{values_str}')\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT identifier, id as business_id\n",
    "    FROM public.businesses\n",
    "    WHERE {in_clause}\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql_query(query, db_connection)\n",
    "    print(f\"Found {len(df)} businesses matching the corp_nums/identifiers\")\n",
    "    print(PRINT_DIVIDER)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_lear_filings(db_connection: Any, business_ids_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query Filings table for business_ids and filter for Source = 'LEAR'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if business_ids_df.empty:\n",
    "            print(\"Empty business_ids_df\")\n",
    "            print(PRINT_DIVIDER)\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        business_id_list = business_ids_df['business_id'].tolist()\n",
    "        value_holders = ','.join(str(val) for val in business_id_list)\n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT business_id, filing_date, filing_type, status, source\n",
    "        FROM public.filings\n",
    "        WHERE business_id IN ({value_holders})\n",
    "        AND source = 'LEAR'\n",
    "        ORDER BY business_id, filing_date DESC\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_sql_query(query, db_connection)\n",
    "        print(f\"Found {len(df)} filings with source  'LEAR'\")\n",
    "        print(PRINT_DIVIDER)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting LEAR filings: {e}\")\n",
    "        print(PRINT_DIVIDER)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def process_found_lear_filings(filings_df: pd.DataFrame, business_ids_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Group filing types with status by business_id and get the latest filing date\n",
    "    Returns DataFrame with identifiers, lear_filings_found, last_filing_date\n",
    "    \"\"\"\n",
    "    if filings_df.empty:\n",
    "        return pd.DataFrame(columns=['identifiers', 'lear_filings_found', 'last_filing_date'])\n",
    "    \n",
    "    filings_df['formatted_filing_type'] = filings_df['filing_type'].apply(camel_to_title_case)\n",
    "    # Create filing type with status string\n",
    "    filings_df['filing_with_status'] = filings_df['formatted_filing_type'] + ' (' + filings_df['status'] + ')'\n",
    "\n",
    "    # Group by business_id and aggregate\n",
    "    aggregated = filings_df.groupby('business_id').agg({\n",
    "        'filing_with_status': lambda x: ', '.join(sorted(set(x))),  # Unique filing types with status joined by comma\n",
    "        'filing_date': 'max'  # Latest filing date\n",
    "    }).reset_index()\n",
    "\n",
    "\n",
    "    aggregated.columns = ['business_id', 'lear_filings_found', 'last_filing_date']\n",
    "\n",
    "    result_df = business_ids_df.merge(aggregated, on='business_id', how='inner')\n",
    "    result_df['last_filing_date'] = pd.to_datetime(result_df['last_filing_date'].dt.date)\n",
    "    \n",
    "    result_df = result_df.rename(columns={'identifier': COLUMN_NAMES['corp_num']})\n",
    "    result_df = result_df[[COLUMN_NAMES['corp_num'], 'lear_filings_found', 'last_filing_date']]\n",
    "\n",
    "    print(f\"Processed {len(result_df)} businesses with LEAR filings\")\n",
    "    print(PRINT_DIVIDER)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def camel_to_title_case(camel_str):\n",
    "    \"\"\"Convert camelCase to Title Case (e.g., 'annualReport' -> 'Annual Report')\"\"\"\n",
    "    import re\n",
    "    # Insert space before uppercase letters that follow lowercase letters\n",
    "    result = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', camel_str)\n",
    "    # Capitalize first letter of each word\n",
    "    return result.title()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d89ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put LEAR filings and the latest filing date into the main dataframe\n",
    "try:\n",
    "    business_ids_df = get_business_ids(lear_engine, corp_num_column_values)\n",
    "    lear_filings_df = get_lear_filings(lear_engine, business_ids_df)\n",
    "    lear_results_df = process_found_lear_filings(lear_filings_df, business_ids_df)\n",
    "    with pd.option_context(\"display.max_rows\", None):\n",
    "        display(lear_results_df)\n",
    "    print(PRINT_DIVIDER)\n",
    "\n",
    "    current_df = migration_df.copy(deep=True)\n",
    "    current_df = current_df.merge(\n",
    "        lear_results_df,\n",
    "        on=COLUMN_NAMES['corp_num'],\n",
    "        how='left'\n",
    "    )\n",
    "    current_df['lear_filings_found'] = current_df['lear_filings_found'].fillna('')\n",
    "    current_df['last_filing_date'] = current_df['last_filing_date'].astype(str).replace('NaT', '')\n",
    "\n",
    "    current_df = current_df.rename(\n",
    "        columns={\n",
    "            'lear_filings_found': COLUMN_NAMES['filings'],\n",
    "            'last_filing_date': COLUMN_NAMES['filing_date']\n",
    "        }\n",
    "    )\n",
    "\n",
    "    with pd.option_context(\"display.max_rows\", None):\n",
    "        display(current_df)\n",
    "except Exception as e:\n",
    "    print(f\"Error putting LEAR filings data into the main dataframe:\\n {e}\")\n",
    "    print(PRINT_DIVIDER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd6cca3",
   "metadata": {},
   "source": [
    "### Save the final dataframe to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5befa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl.styles import Font, Alignment\n",
    "\n",
    "def format_and_save_excel(df: pd.DataFrame, file_save_path: str = 'output.xlsx', sheet_name: str = 'Sheet1') -> None:\n",
    "    \"\"\"Save to Excel file with basic freeze and alignment.\"\"\"\n",
    "    \n",
    "    # Generate unique filename with date and incremental number\n",
    "    file_save_path = generate_unique_filename(file_save_path)\n",
    "\n",
    "    with pd.ExcelWriter(file_save_path, engine='openpyxl') as writer:\n",
    "        # write the dataframe\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "        # get the workbook and worksheet\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "\n",
    "        # Freeze the 1st row\n",
    "        worksheet.freeze_panes = 'A2'\n",
    "\n",
    "        # Make header bold\n",
    "        for cell in worksheet[1]:  # First row (header)\n",
    "            cell.font = Font(bold=True)\n",
    "            cell.alignment = Alignment(horizontal='left')\n",
    "    \n",
    "        # Left align all other cells\n",
    "        for row in worksheet.iter_rows(min_row=2):  # Skip header row\n",
    "            for cell in row:\n",
    "                cell.alignment = Alignment(horizontal='left')\n",
    "        \n",
    "        # Adjust column widths\n",
    "        for column in worksheet.columns:\n",
    "            max_length = 0\n",
    "            column_letter = column[0].column_letter\n",
    "\n",
    "            for cell in column:\n",
    "                try:\n",
    "                    if len(str(cell.value)) > max_length:\n",
    "                        max_length = len(str(cell.value))\n",
    "                except:\n",
    "                    pass\n",
    "            adjusted_width = max_length + 2\n",
    "            worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "        \n",
    "        print(f\"DataFrame saved to {file_save_path} with frozen header and left alignment\")\n",
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_unique_filename(original_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate unique filename with date and incremental number if file exists.\n",
    "    Example: 'output.xlsx' -> 'output_20250626.xlsx' -> 'output_20250626_02.xlsx'\n",
    "    \"\"\"\n",
    "    # Get directory, filename, and extension\n",
    "    directory = os.path.dirname(original_path)\n",
    "    filename = os.path.basename(original_path)\n",
    "    name, ext = os.path.splitext(filename)\n",
    "\n",
    "    # Add today's date\n",
    "    today = datetime.now().strftime('%Y%m%d')\n",
    "    new_filename = f\"{name}_{today}{ext}\"\n",
    "    new_path = os.path.join(directory, new_filename)\n",
    "\n",
    "    # If file doesn't exist, return the new path\n",
    "    if not os.path.exists(new_path):\n",
    "        return new_path\n",
    "    \n",
    "    # If file exists, add incremental number\n",
    "    counter = 2\n",
    "    while True:\n",
    "        incremental_filename = f\"{name}_{today}_{counter:02d}{ext}\"\n",
    "        incremental_path = os.path.join(directory, incremental_filename)\n",
    "        \n",
    "        if not os.path.exists(incremental_path):\n",
    "            return incremental_path\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        # Safety check to prevent infinite loop\n",
    "        if counter > 999:\n",
    "            raise Exception(\"Too many files with the same name pattern\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec04b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export formatted Excel file\n",
    "current_df[COLUMN_NAMES['date']] = current_df[COLUMN_NAMES['date']].astype(str)\n",
    "output_path = f\"{OUTPUT_FOLDER}/test_result.xlsx\"\n",
    "format_and_save_excel(current_df, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
